{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BgR7Xu2R79kN",
        "h-D1cXwY8P5d"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4L6ts9Nd7uD5"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install underthesea"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define function to process data"
      ],
      "metadata": {
        "id": "BgR7Xu2R79kN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from underthesea import word_tokenize\n",
        "\n",
        "from gensim.models import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "import numpy as np\n",
        "\n",
        "def save_chunked_paragraph_to_file(file_path, content):\n",
        "  with open(file_path, 'w') as file:\n",
        "    file.write(content)\n",
        "\n",
        "def preprocess_paragraph(paragraph):\n",
        "  \"\"\"\n",
        "  Tiền xử lý văn bản: chuyển đổi văn bản thành một danh sách các từ.\n",
        "  Args:\n",
        "  - paragraph: đoạn văn cần tiền xử lý.\n",
        "  Returns:\n",
        "  - tokenized_text: danh sách các từ đã được tiền xử lý.\n",
        "  \"\"\"\n",
        "  # tokenized_text = paragraph.lower().split()\n",
        "  # return tokenized_text\n",
        "  tokenized_text = word_tokenize(paragraph)\n",
        "  return tokenized_text\n",
        "\n",
        "def get_topic_distribution(text, lda_model, dictionary):\n",
        "  \"\"\"\n",
        "  Trả về phân phối chủ đề của một đoạn văn bản.\n",
        "  Args:\n",
        "  - text: đoạn văn bản cần xác định phân phối chủ đề.\n",
        "  - lda_model: mô hình LDA đã được huấn luyện.\n",
        "  - dictionary: từ điển Gensim.\n",
        "  Returns:\n",
        "  - topic_distribution: phân phối chủ đề của đoạn văn bản.\n",
        "  \"\"\"\n",
        "  tokenized_text = preprocess_paragraph(text)\n",
        "  bow_vector = dictionary.doc2bow(tokenized_text)\n",
        "  topic_distribution = lda_model.get_document_topics(bow_vector)\n",
        "  return np.array([topic_prob[1] for topic_prob in topic_distribution])\n",
        "\n",
        "def kl_divergence(p, q):\n",
        "  #Nếu P và Q càng giống nhau, KL divergence càng nhỏ.\n",
        "  return np.exp(-np.sum(p * np.log(p / q)))\n",
        "\n",
        "def compare_topic_distributions(topic_dist1, topic_dist2, threshold):\n",
        "  \"\"\"\n",
        "  So sánh phân phối chủ đề giữa hai đoạn văn bản.\n",
        "  Args:\n",
        "  - topic_dist1: phân phối chủ đề của đoạn văn bản thứ nhất.\n",
        "  - topic_dist2: phân phối chủ đề của đoạn văn bản thứ hai.\n",
        "  - threshold: ngưỡng để xác định sự khác biệt lớn.\n",
        "  Returns:\n",
        "  - is_different: True nếu có sự khác biệt lớn, False nếu không.\n",
        "  \"\"\"\n",
        "  kl_score = kl_divergence(topic_dist1, topic_dist2)\n",
        "  # print(kl_score)\n",
        "  if kl_score > threshold:\n",
        "      return True\n",
        "  else:\n",
        "      return False"
      ],
      "metadata": {
        "id": "wKEfpcoj7-F6"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define function to chunk"
      ],
      "metadata": {
        "id": "h-D1cXwY8P5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from underthesea import sent_tokenize\n",
        "\n",
        "#is_preview dùng để in ra console trong trường hợp không muốn xuất file output\n",
        "def chunk_document(file_path, target_dir, window_size = 2, threshold = 0.995, is_preview = False):\n",
        "  #Đọc file\n",
        "  document = ''\n",
        "\n",
        "  with open(file_path, 'r') as file:\n",
        "    document = file.read()\n",
        "\n",
        "  #Chia văn bản thành các câu:\n",
        "  sentences = sent_tokenize(document)\n",
        "\n",
        "  index = 0\n",
        "  paragraph_size = 0\n",
        "  chunk_index = 0\n",
        "\n",
        "  while index < len(sentences):\n",
        "    paragraph_size += window_size\n",
        "\n",
        "    # Lấy các câu trong slide window\n",
        "    paragraph1 = \" \".join(sentences[index:index + paragraph_size])\n",
        "\n",
        "    if index + paragraph_size > len(sentences):\n",
        "      if is_preview:\n",
        "        print(\"Length paragraph: \" + str(paragraph_size) + \" sentences\")\n",
        "        print(paragraph1)\n",
        "        print(\"----------------------------------------------\")\n",
        "      else:\n",
        "        chunk_file = target_dir + '/chunk' + str(chunk_index) + '.txt'\n",
        "        save_chunked_paragraph_to_file(chunk_file, paragraph1)\n",
        "      chunk_index += 1\n",
        "      break\n",
        "\n",
        "    paragraph2 = \"\"\n",
        "    if paragraph_size >= window_size * 2:\n",
        "      paragraph2 = \" \".join(sentences[index + paragraph_size: index + paragraph_size + window_size * 2])\n",
        "    else:\n",
        "      paragraph2 = \" \".join(sentences[index + paragraph_size: index + paragraph_size + window_size])\n",
        "\n",
        "\n",
        "    # Tiền xử lý văn bản\n",
        "    preprocessed_documents = [preprocess_paragraph(paragraph1), preprocess_paragraph(paragraph2)]\n",
        "\n",
        "    # Tạo từ điển\n",
        "    dictionary = Dictionary(preprocessed_documents)\n",
        "\n",
        "    # Tạo BoW Corpus\n",
        "    bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]\n",
        "\n",
        "    # Huấn luyện mô hình LDA\n",
        "    lda_model = LdaModel(bow_corpus, num_topics=2, id2word=dictionary, passes=20)\n",
        "\n",
        "    # So sánh phân phối chủ đề giữa các đoạn văn bản\n",
        "    topic_distribution1 = get_topic_distribution(paragraph1, lda_model, dictionary)\n",
        "    topic_distribution2 = get_topic_distribution(paragraph2, lda_model, dictionary)\n",
        "\n",
        "    # Kiểm tra sự khác biệt giữa hai đoạn văn bản\n",
        "    if compare_topic_distributions(topic_distribution1, topic_distribution2, threshold):\n",
        "      if is_preview:\n",
        "        print(\"Length paragraph: \" + str(paragraph_size) + \" sentences\")\n",
        "        print(paragraph1)\n",
        "        print(\"----------------------------------------------\")\n",
        "      else:\n",
        "        #Lưu file\n",
        "        chunk_file = target_dir + '/chunk' + str(chunk_index) + '.txt'\n",
        "        save_chunked_paragraph_to_file(chunk_file, paragraph1)\n",
        "      chunk_index += 1\n",
        "      # Tiến tới so sánh tiếp theo\n",
        "      index += paragraph_size\n",
        "      paragraph_size = 0\n"
      ],
      "metadata": {
        "id": "dqoAWEql8PQ4"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample"
      ],
      "metadata": {
        "id": "uJw7U5qs9PuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def chunk_all_file_in_dir(src_dir, target_dir):\n",
        "  files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n",
        "  for file in files:\n",
        "    file_path = src_dir + '/' + file\n",
        "    file_name = os.path.splitext(file)[0]\n",
        "    output_path = target_dir + '/' + file_name\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    chunk_document(file_path, output_path)\n",
        "\n",
        "chunk_all_file_in_dir('input', 'output')\n",
        "\n"
      ],
      "metadata": {
        "id": "A70-_5_pMyRX"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_document('input/test2.txt', 'output/test2', is_preview=True)"
      ],
      "metadata": {
        "id": "X4PPupzGROf_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}